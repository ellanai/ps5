---
title: "Problem Set 5"
author: "Evy Lanai & Alberto Saldarriaga"
date: "November 9, 2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1: Evy Lanai, ellanai
    - Partner 2: Alberto Saldarriaga, asaldarriagav
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*EL\*\* \*\*AS\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*1\*\* Late coins left after submission: \*\*1\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from datetime import datetime

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
```

  ```{python}
url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')
# Test to make sure it looks right
soup.text[0:50]
  ```

```{python}
# Extract the title of the enforcement action
tag = soup.find_all('a')
titles = list(filter(lambda tag: tag.get('href') and '/fraud/enforcement/' in tag['href'], soup.find_all('a')))
# There were 3 items pulled in that I don't want, so I'm taking them out
titles = titles[3: ]

# Extract the date of the enforcement action
date_divs = soup.find_all('div', class_='font-body-sm margin-top-1')
dates = [div.find('span', class_='text-base-dark padding-right-105').text for div in date_divs if div.find('span', class_='text-base-dark padding-right-105')]

# Extract the category of the enforcement action
date_divs = soup.find_all('div', class_='font-body-sm margin-top-1')
categories = [div.find('li').text for div in date_divs if div.find('li')]

# Extract the link associated with the enforcement action
links = [tag['href'] for tag in titles]
# Add domain
links = ['https://oig.hhs.gov' + link for link in links]
print(links[0])
```

```{python}
# Make a data frame
enforcement_action_df = pd.DataFrame({
  "Title": titles,
  "Date": dates,
  "Category": categories,
  "Link": links
})
print(enforcement_action_df.head())
```

### 2. Crawling (PARTNER 1)

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/macomb-county-doctor-and-pharmacist-agree-to-pay-700948-to-settle-false-claims-act-allegations/"
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')

li_tags = soup.find_all('li')
for li in li_tags:
    if 'Agency:' in li.get_text():
        # Extract text after "Agency:"
        agency = li.get_text().split('Agency:')[-1].strip()  
        agencies.append(agency)
```



```{python}
# Create a for loop to extract agency from each link in list
agencies = []

for link in links:
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'html.parser')

    li_tags = soup.find_all('li')
    for li in li_tags:
      if 'Agency:' in li.get_text():
          # Extract text after "Agency:"
          agency = li.get_text().split('Agency:')[-1].strip()  
          agencies.append(agency)

# Update data frame
enforcement_action_df["Agency"] = agencies
print(enforcement_action_df.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

First, we define a function with parameters 'year' and 'month'. This will allow us to fetch enforcement actions from a specific date (starting year and month as inputs) onward.

Second, we constrain the starting year as >= 2013, since only enforcement actions after 2013 are listed. Otherwise, the function prints a message saying 'Please, enter a year >= 2013'

Third, we set up our general scrapping code by identifying: (i) base_url, (ii) lists to store my data (titles, dates, categories, links and agencies), (iii) only links in the first page, and (iv) an end_date to avoid unnecessary scraping past the current date.

As a fourth step, we create a loop that will allow us to go through all the pages. We use a while loop because it allows the scraper to keep fetching and processing pages until it reaches a specific condition, such as the end of the data or a date limit. A for loop, on the other hand, is not very useful in this case, because we don't have a specific range of observations.

As a fifth step, we start extracting data (all links and dates related to enforcement actions), filtering information only within our desired time range, and convert the extracted date in datetime format.

Just as a caveat, even when our general/initial loop was a While True, to extract the information within each webpage we replicate what we did in **Step 1**, and use a for loop. 

As a sixth step, given that we are crawling through multiple webpages, we increment the page number to move to the next page and add a 1-second delay to avoid overwhelming the server. Here we use the .sleep() function.

Finally, we create a dataframe with all the information retrieved and saved it as a csv file named as 'enforcement_actions_year_month.csv'

how many times I'm going through ny while loop

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
def scrape_enforcement_actions(year, month):
    # Validate the start date
    if year < 2013:
        print("Please enter a start date year >= 2013")
        return
    
    # Initialize variables
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    all_titles, all_dates, all_categories, all_links, all_agencies = [], [], [], [], []
    page_num = 1
    
    counter = 0
    
    while True:  # Loop through pages
        counter += 1
        print(counter)

        # Record start time for the page load
        page_start_time = time.time()

        url = f"{base_url}?page={page_num}"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract titles, dates, and links on the page
        titles = list(filter(lambda tag: tag.get('href') and '/fraud/enforcement/' in tag['href'], soup.find_all('a')))[3:]
        date_divs = soup.find_all('div', class_='font-body-sm margin-top-1')

        # Check if the page has no new data
        if not titles:
            break
        
        # Define the start date for comparison
        start_date = datetime(year, month, 1)
        
        for i in range(len(titles)):
            title = titles[i].text.strip()
            date_text = date_divs[i].find('span', class_='text-base-dark padding-right-105').text.strip()
            category = date_divs[i].find('li').text.strip() if date_divs[i].find('li') else ""
            link = 'https://oig.hhs.gov' + titles[i]['href']
                        
            # Extract year and month only from the date
            action_date = datetime.strptime(date_text, "%B %d, %Y")
            action_year_month = action_date.replace(day=1)

            # Break the loop if action_date is earlier than the specified start date
            if action_year_month < start_date:
                break

            # Only add actions from the specified start year and month onward
            all_titles.append(title)
            all_dates.append(action_date)
            all_categories.append(category)
            all_links.append(link)

            # Fetch agency details for each enforcement action
            agency = "N/A"
            agency_response = requests.get(link)
            agency_soup = BeautifulSoup(agency_response.text, 'html.parser')
            
            # Find the agency within <li> tags that contain "Agency:"
            for li in agency_soup.find_all('li'):
                if 'Agency:' in li.get_text():
                    agency = li.get_text().split('Agency:')[-1].strip()
                    break  # Stop after finding the first match
            
            all_agencies.append(agency)

        # If the action_date on this page is earlier than start_date, stop scraping
        if action_year_month < start_date:
            break

        # Move to the next page
        page_num += 1
        time.sleep(1)

        # Record end time for the page load and calculate elapsed time
        page_end_time = time.time()
        page_load_time = page_end_time - page_start_time
        print(f"Time taken to change page {page_num}: {page_load_time:.2f} seconds")

    # Save to DataFrame with Date in datetime format
    enforcement_action_df = pd.DataFrame({
        "Title": all_titles,
        "Date": all_dates,
        "Category": all_categories,
        "Link": all_links,
        "Agency": all_agencies
    })

    # Save DataFrame to CSV
    file_name = f"enforcement_actions_{start_date.year}_{start_date.month}.csv"
    enforcement_action_df.to_csv(file_name, index=False)
    print(f"Data saved to {file_name}")
    
    return enforcement_action_df

    return enforcement_action_df

# Define the year and month for filtering
year = 2023
month = 1

# Record the start time
start_time = time.time()

# Run the dynamic scraper function for all pages
df = scrape_enforcement_actions(year, month)

# Record the end time
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time
elapsed_time_m = elapsed_time / 
print(f"Scraping completed in {elapsed_time_m:.2f} minutes")

# Print the total number of enforcement actions
print(f"Total enforcement actions since January 2023: {len(df)}")

# Find the earliest enforcement action and print its details
earliest_action = df.sort_values(by="Date").iloc[0]
print("Earliest enforcement action:")
print(earliest_action)
```

* c. Test Partner's Code (PARTNER 1)

```{python}
def scrape_enforcement_actions(year, month):
    # Validate the start date
    if year < 2013:
        print("Please enter a start date year >= 2013")
        return
    
    # Initialize variables
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    all_titles, all_dates, all_categories, all_links, all_agencies = [], [], [], [], []
    page_num = 1
    
    counter = 0
    
    while True:  # Loop through pages
        counter += 1
        print(counter)

        # Record start time for the page load
        page_start_time = time.time()

        url = f"{base_url}?page={page_num}"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract titles, dates, and links on the page
        titles = list(filter(lambda tag: tag.get('href') and '/fraud/enforcement/' in tag['href'], soup.find_all('a')))[3:]
        date_divs = soup.find_all('div', class_='font-body-sm margin-top-1')

        # Check if the page has no new data
        if not titles:
            break
        
        # Define the start date for comparison
        start_date = datetime(year, month, 1)
        
        for i in range(len(titles)):
            title = titles[i].text.strip()
            date_text = date_divs[i].find('span', class_='text-base-dark padding-right-105').text.strip()
            category = date_divs[i].find('li').text.strip() if date_divs[i].find('li') else ""
            link = 'https://oig.hhs.gov' + titles[i]['href']
                        
            # Extract year and month only from the date
            action_date = datetime.strptime(date_text, "%B %d, %Y")
            action_year_month = action_date.replace(day=1)

            # Break the loop if action_date is earlier than the specified start date
            if action_year_month < start_date:
                break

            # Only add actions from the specified start year and month onward
            all_titles.append(title)
            all_dates.append(action_date)
            all_categories.append(category)
            all_links.append(link)

            # Fetch agency details for each enforcement action
            agency = "N/A"
            agency_response = requests.get(link)
            agency_soup = BeautifulSoup(agency_response.text, 'html.parser')
            
            # Find the agency within <li> tags that contain "Agency:"
            for li in agency_soup.find_all('li'):
                if 'Agency:' in li.get_text():
                    agency = li.get_text().split('Agency:')[-1].strip()
                    break  # Stop after finding the first match
            
            all_agencies.append(agency)

        # If the action_date on this page is earlier than start_date, stop scraping
        if action_year_month < start_date:
            break

        # Move to the next page
        page_num += 1
        time.sleep(1)

        # Record end time for the page load and calculate elapsed time
        page_end_time = time.time()
        page_load_time = page_end_time - page_start_time
        print(f"Time taken to change page {page_num}: {page_load_time:.2f} seconds")

    # Save to DataFrame with Date in datetime format
    enforcement_action_df = pd.DataFrame({
        "Title": all_titles,
        "Date": all_dates,
        "Category": all_categories,
        "Link": all_links,
        "Agency": all_agencies
    })

    # Save DataFrame to CSV
    file_name = f"enforcement_actions_{start_date.year}_{start_date.month}.csv"
    enforcement_action_df.to_csv(file_name, index=False)
    print(f"Data saved to {file_name}")
    
    return enforcement_action_df

    return enforcement_action_df

# Define the year and month for filtering
year = 2021
month = 1

# Record the start time
start_time = time.time()

# Run the dynamic scraper function for all pages
df = scrape_enforcement_actions(year, month)

# Record the end time
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time
elapsed_time_m = elapsed_time / 60
print(f"Scraping completed in {elapsed_time_m:.2f} minutes")

# Print the total number of enforcement actions
print(f"Total enforcement actions since January 2023: {len(df)}")

# Find the earliest enforcement action and print its details
earliest_action = df.sort_values(by="Date").iloc[0]
print("Earliest enforcement action:")
print(earliest_action)
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
path = r"C:\Users\msald\OneDrive\Escritorio\UChicago\3. Classes\2425\1. Autum\2. PythonII\ps5\enforcement_actions_2021_1.csv"

df_2021 = pd.read_csv(path)

# Change the 'Date' column to datetime format
df_2021['Date'] = pd.to_datetime(df_2021['Date'])

# Plot the line chart using Altair
alt.Chart(df_2021).mark_line(color='#8dd3c7').transform_timeunit(
    YearMonth='yearmonth(Date)'
).transform_aggregate(
    count_ea='count()',
    groupby=['YearMonth']
).encode(
    x=alt.X(
        'YearMonth:T',
        title='Month and Year (2021-2024)',
        axis=alt.Axis(
            format="%b %Y",
            labelAngle=-45,
            tickCount='month',
            grid=True,
            gridColor='lightgrey',
            gridDash=[2, 2]  
        )
    ),
    y=alt.Y('count_ea:Q', title='Number of Enforcement Actions'),
    tooltip=['YearMonth:T', 'count_ea:Q']
).properties(
    title='Number of Enforcement Actions Over Time (Aggregated by Month and Year)',
    width=600,
    height=300
)
```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
# Plot 1
# Plot Criminal and Civil Actions vs. State Enforcement Agencies
df_CCA_SEA = df[df["Category"].isin(["Criminal and Civil Actions", "State Enforcement Agencies"])]
# Change Date column to just month can year
df_CCA_SEA["Date"] = df_CCA_SEA["Date"].dt.to_period("M").astype(str)
# Group number of actions by month and year
df_CCA_SEA_grouped = df_CCA_SEA.groupby(["Category", "Date"]).size().reset_index(name="Count")
# Plot
CCA_SEA_chart = alt.Chart(df_CCA_SEA_grouped).mark_line().encode(
    x=alt.X("Date:T", title="Month and Year", axis=alt.Axis(format="%b %Y", labelAngle=-45)),
    y="Count:Q",
    color="Category:N"
).properties(
    title="Criminal and Civil Actions vs. State Enforcement Agencies Over Time",
    width=350
)
```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
# Cleaning the Agency column
# Filter out U.S.-level entries, N/A entries, district-level agencies, and department-level agencies
df_state = df[~df["Agency"].str.contains("U.S.", case=False) &
              (df["Agency"] != "N/A") &
              ~df["Agency"].str.contains("District", case=False) &
              ~df["Agency"].str.contains("Department", case=False)]

# Take out 'State of' for each state
df_state["Agency"] = df_state["Agency"].str.replace("State of", "", regex=False)
# Take out 'Attorney General' for each state
df_state["Agency"] = df_state["Agency"].str.replace("Attorney General", "", regex=False)
# Adjust typo in one case of Nevada
df_state["Agency"] = df_state["Agency"].str.replace("Attorney Genera", "", regex=False)

# Individual case data cleaning
# North and South Carolina or likely district-level
df_state = df_state[~df_state["Agency"].str.contains("North California", case=False)]
df_state = df_state[~df_state["Agency"].str.contains("South California", case=False)]
# Washington and Washington State are the same
df_state["Agency"] = df_state["Agency"].str.replace("Washington State", "Washington")

# Remove spaces
# List of states with multiple words
exceptions = [
    "New York", "New Jersey", "New Mexico", "Rhode Island", "North Carolina",
    "South Carolina", "West Virginia", "New Hampshire"
]
# Strip whitespace for all other states
df_state["Agency"] = df_state["Agency"].apply(lambda x: x.strip() if x not in exceptions else x)

# Create df for grouped enforcement actions by state
df_state_grouped = df_state.groupby("Agency").size().reset_index(name="Count")
# Rename 'Agency' to 'NAME' for merge
df_state_grouped.rename(columns={"Agency": "NAME"}, inplace=True)
# Rename Hawaii for merge
# Using .iloc[] to access row 8 and the first column (index 0)
df_state_grouped.iloc[8, 0] = "Hawaii"
```

```{python}
import geopandas as gpd
import fiona
import pyogrio
shp = gpd.read_file("/Users/evylanai/MPP Autumn '24/Python II/cb_2018_us_state_20m/cb_2018_us_state_20m.shp")
```

```{python}
# Merge grouped state df with .shp file
shp_state_actions = pd.merge(shp, df_state_grouped, on="NAME", how="left")
# Exlcuding some states for visuals + P.R. because it's not a state
exclude_states = ["Alaska", "Hawaii", "Puerto Rico"]
shp_state_actions = shp_state_actions[~shp_state_actions["NAME"].isin(exclude_states)]

import matplotlib.pyplot as plt

# Plot the choropleth
fig, ax = plt.subplots(1, 1, figsize=(12, 8))
shp_state_actions.plot(
    column="Count",
    cmap="YlOrRd", 
    linewidth=0.8,
    ax=ax,
    edgecolor="0.8",
    legend=True,
    # Add shading for states with NaN values
    missing_kwds={
        "color": "lightgray",
        "edgecolor": "0.7",  
    }
)

# Customize the plot
ax.set_title("Number of Actions Enforced by State Agencies since Jan. 2021", fontsize=15)
ax.set_axis_off()

# Add note -- code help from ChatGPT
ax.annotate(
    'NOTE: Alaska and Hawaii not included \nfor visualization purposes. Alaska has no \nactions recorded, and Hawaii has one.', 
    xy=(0.5, -0.06),  # Positioning the note (x, y)
    xycoords='axes fraction',  # Coordinates relative to the axes
    ha='right',  # Horizontal alignment
    va='center',  # Vertical alignment
    fontsize=12,  # Font size
    color='black',  # Font color
    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.5')  # Background box
)
plt.show()

```

### 2. Map by District (PARTNER 2)

```{python}
import geopandas as gpd
import fiona
import pyogrio
from shapely import wkt
import matplotlib.pyplot as plt
```

```{python}
# Load the US Attorney df
path = r"C:\Users\msald\OneDrive\Escritorio\UChicago\3. Classes\2425\1. Autum\2. PythonII\ps5\US_Attorney_Districts_Shapefile_simplified_20241108.csv"

us_attorney_df = pd.read_csv(path)

## Rename our interest variable
us_attorney_df.rename(columns={'Judicial District ': 'District'}, inplace=True)

## Rename one of the districts so it matches our original df
us_attorney_df['District'] = us_attorney_df['District'].replace({
    'District of District of Columbia': 'District of Columbia'
})

# Apply a function to create a new column in your DataFrame
def extract_district(agency):
    if isinstance(agency, str):
        if "Attorney's Office," in agency:
            return agency.split("Attorney's Office,")[-1].strip()
        elif 'Attorney General,' in agency:
            return agency.split('Attorney General,')[-1].strip()
        elif 'Western District of Oklahoma' in agency:
            return 'Western District of Oklahoma'
        elif 'Western District of Pennsylvania' in agency:
            return 'Western District of Pennsylvania'
        elif 'District of Connecticut' in agency:
            return 'District of Connecticut'
        elif 'District of New Jersey' in agency:
            return 'District of New Jersey'
        elif 'Eastern District of New York' in agency:
            return 'Eastern District of New York'
        elif 'Eastern District of North Carolina' in agency:
            return 'Eastern District of North Carolina'
    return None

df_2021['District'] = df_2021['Agency'].apply(extract_district)

## Filter out only the Judicial District
df_2021_districts = df_2021[df_2021['District'].notna()]

## Renaming remaining districts
district_replacements = {
    'District of Massachusetts †††': 'District of Massachusetts',
    'District of South Dakota ††': 'District of South Dakota',
    'Southern District of New York ††': 'Southern District of New York',
    'Western District of Virginia ††': 'Western District of Virginia',
    'District of Idaho Boise': 'District of Idaho'
}

df_2021_districts['District'] = df_2021_districts['District'].replace(district_replacements)

df_2021_districts_2 = df_2021_districts.groupby('District').size().reset_index(name = 'count_ea')

# Merge with US Attorney's database
merge_partner2 = pd.merge(us_attorney_df, df_2021_districts_2, on='District', how='left')

# Transform my df y geodataframe format
merge_partner2['the_geom'] = merge_partner2['the_geom'].apply(lambda x: wkt.loads(x) if isinstance(x, str) else x)
print(merge_partner2['the_geom'].apply(type).value_counts())

merge_partner2_geo = gpd.GeoDataFrame(merge_partner2, geometry='the_geom')


```

```{python}
# Finally, we create the choropleth
exclude_states = ["District of Alaska", 
"District of Hawaii", 
"District of Puerto Rico",
"District of US Virgin Islands",
"District of Guam",
"District of Northern Marianas Islands"]

merge_partner2_geo = merge_partner2_geo[~merge_partner2_geo["District"].isin(exclude_states)]

# Plot the choropleth
fig, ax = plt.subplots(1, 1, figsize=(12, 8))
merge_partner2_geo.plot(
    column="count_ea",
    cmap="YlOrRd", 
    linewidth=0.8,
    ax=ax,
    edgecolor="0.8",
    legend=True,
    # Add shading for states with NaN values
    missing_kwds={
        "color": "lightgray",
        "edgecolor": "0.7",  
    }
)

# Customize the plot
ax.set_title("Number of Enforcement Actions by U.S. Attorney District since Jan. 2021", fontsize=15)
ax.set_axis_off()

# Add note -- code help from ChatGPT
ax.annotate(
    'NOTE: Six Judicial Districts were removed due to: \n(i) only 1 enforcement action or NA value, and \n(ii) visualization purposes.', 
    xy=(0.5, -0.06),  # Positioning the note (x, y)
    xycoords='axes fraction',  # Coordinates relative to the axes
    ha='right',  # Horizontal alignment
    va='center',  # Vertical alignment
    fontsize=12,  # Font size
    color='black',  # Font color
    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.5')  # Background box
)
plt.show()
```


## Extra Credit

### 1. Merge zip code shapefile with population
```{python}
#Load the shapefile
path = r"/Users/evylanai/MPP Autumn '24/Python II/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
zipcodes = gpd.read_file(path)

#Load the census data 2020
path = r"/Users/evylanai/MPP Autumn '24/Python II/DECENNIALDHC2020.P1_2024-11-09T173918/DECENNIALDHC2020.P1-Data.csv"
census_df_2020 = pd.read_csv(path)
```

```{python}
#print(zipcodes.columns)
#print(zipcodes.head(10))

#print(census_df_2020.columns)
#print(census_df_2020.head(10))

def extract_zipcode(zipcode):
    if isinstance(zipcode, str):
        if "ZCTA5" in zipcode:
            return zipcode.split("ZCTA5")[-1].strip()
    return None

census_df_2020['ZCTA5'] = census_df_2020['NAME'].apply(extract_zipcode)

merge_extracredit = pd.merge(zipcodes, census_df_2020[['ZCTA5', 'P1_001N']], on='ZCTA5', how='left')

```

```{python}
#print(merge_extracredit.head(10))
print(merge_extracredit['P1_001N'].dtype)
#test = us_attorney_df.groupby('ZCTA5').size().reset_index(name='count')


```

### 2. Conduct spatial join
```{python}
# Import district data
districts_csv = pd.read_csv(r"/Users/evylanai/MPP Autumn '24/Python II/US_Attorney_Districts_Shapefile_simplified_20241110.csv")
# Transform my df y geodataframe format
from shapely import wkt

districts_csv['the_geom'] = districts_csv['the_geom'].apply(lambda x: wkt.loads(x) if isinstance(x, str) else x)
# Turn to GeoDataFrame
districts_shp = gpd.GeoDataFrame(districts_csv, geometry='the_geom')

# Conduct spatial join
pop_byzip = gpd.sjoin(merge_extracredit, districts_shp, how="inner", predicate="intersects")
```

```{python}
# Turn population variable to numeric 
pop_byzip['P1_001N'] = pd.to_numeric(pop_byzip['P1_001N'], errors='coerce')
# Aggregate population by district
pop_bydistrict = pop_byzip.groupby("Judicial District ")["P1_001N"].sum().reset_index()
# Merge back to get geospatial data
districts_shp = districts_shp.merge(pop_bydistrict, on="Judicial District ")
```

```{python}

```

### 3. Map the action ratio in each district

```{python}

```



