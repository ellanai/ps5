---
title: "Problem Set 5"
author: "Evy Lanai & Alberto Saldarriaga"
date: "November 9, 2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1: Evy Lanai, ellanai
    - Partner 2: Alberto Saldarriaga, asaldarriagav
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*EL\*\* \*\*AS\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*1\*\* Late coins left after submission: \*\*1\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from datetime import datetime

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
```

  ```{python}
url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')
# Test to make sure it looks right
soup.text[0:50]
  ```

```{python}
# Extract the title of the enforcement action
tag = soup.find_all('a')
titles = list(filter(lambda tag: tag.get('href') and '/fraud/enforcement/' in tag['href'], soup.find_all('a')))
# There were 3 items pulled in that I don't want, so I'm taking them out
titles = titles[3: ]

# Extract the date of the enforcement action
date_divs = soup.find_all('div', class_='font-body-sm margin-top-1')
dates = [div.find('span', class_='text-base-dark padding-right-105').text for div in date_divs if div.find('span', class_='text-base-dark padding-right-105')]

# Extract the category of the enforcement action
date_divs = soup.find_all('div', class_='font-body-sm margin-top-1')
categories = [div.find('li').text for div in date_divs if div.find('li')]

# Extract the link associated with the enforcement action
links = [tag['href'] for tag in titles]
# Add domain
links = ['https://oig.hhs.gov' + link for link in links]
print(links[0])
```

```{python}
# Make a data frame
enforcement_action_df = pd.DataFrame({
  "Title": titles,
  "Date": dates,
  "Category": categories,
  "Link": links
})
print(enforcement_action_df.head())
```

### 2. Crawling (PARTNER 1)

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/macomb-county-doctor-and-pharmacist-agree-to-pay-700948-to-settle-false-claims-act-allegations/"
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')

li_tags = soup.find_all('li')
for li in li_tags:
    if 'Agency:' in li.get_text():
        # Extract text after "Agency:"
        agency = li.get_text().split('Agency:')[-1].strip()  
        agencies.append(agency)
```

```{python}
# Create a for loop to extract agency from each link in list
agencies = []

for link in links:
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'html.parser')

    li_tags = soup.find_all('li')
    for li in li_tags:
      if 'Agency:' in li.get_text():
          # Extract text after "Agency:"
          agency = li.get_text().split('Agency:')[-1].strip()  
          agencies.append(agency)

# Update data frame
enforcement_action_df["Agency"] = agencies
print(enforcement_action_df.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

First, we define a function with parameters 'year' and 'month'. This will allow us to fetch enforcement actions from a specific date (starting year and month as inputs) onward.

Second, we constrain the starting year as >= 2013, since only enforcement actions after 2013 are listed. Otherwise, the function prints a message saying 'Please, enter a year >= 2013'

Third, we set up our general scrapping code by identifying: (i) base_url, (ii) lists to store my data (titles, dates, categories, links and agencies), (iii) only links in the first page, and (iv) an end_date to avoid unnecessary scraping past the current date.

As a fourth step, we create a loop that will allow us to go through all the pages. We use a while loop because it allows the scraper to keep fetching and processing pages until it reaches a specific condition, such as the end of the data or a date limit. A for loop, on the other hand, is not very useful in this case, because we don't have a specific range of observations.

As a fifth step, we start extracting data (all links and dates related to enforcement actions), filtering information only within our desired time range, and convert the extracted date in datetime format.

Just as a caveat, even when our general/initial loop was a While True, to extract the information within each webpage we replicate what we did in **Step 1**, and use a for loop. 

As a sixth step, given that we are crawling through multiple webpages, we increment the page number to move to the next page and add a 1-second delay to avoid overwhelming the server. Here we use the .sleep() function.

Finally, we create a dataframe with all the information retrieved and saved it as a csv file named as 'enforcement_actions_year_month.csv'


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
def scrape_enforcement_actions(start_date):
    # Check if the year is valid
    if start_date.year < 2013:
        print("Please enter a start date in the year 2013 or later.")
        return
    
    # Initialize variables
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    all_titles, all_dates, all_categories, all_links, all_agencies = [], [], [], [], []
    page_num = 1
    end_date = datetime.now()
    
    while True:  # Loop to scrape all pages until no more data is found
        # Construct URL for each page
        url = f"{base_url}?page={page_num}"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract data
        titles = list(filter(lambda tag: tag.get('href') and '/fraud/enforcement/' in tag['href'], soup.find_all('a')))[3:]
        date_divs = soup.find_all('div', class_='font-body-sm margin-top-1')
        
        # Check if the page has no new data
        if not titles:
            break
        
        # Process each item on the page
        for i in range(len(titles)):
            title = titles[i].text.strip()
            date = date_divs[i].find('span', class_='text-base-dark padding-right-105').text.strip()
            category = date_divs[i].find('li').text.strip() if date_divs[i].find('li') else ""
            link = 'https://oig.hhs.gov' + titles[i]['href']
            
            # Convert date to datetime format
            action_date = datetime.strptime(date, "%B %d, %Y")
            
            # Only add actions from the specified start date onward
            if action_date >= start_date:
                all_titles.append(title)
                all_dates.append(action_date)  # Store as datetime
                all_categories.append(category)
                all_links.append(link)
                
                # Fetch agency details for each enforcement action
                agency_response = requests.get(link)
                agency_soup = BeautifulSoup(agency_response.text, 'html.parser')
                agency_li = [li.get_text().split('Agency:')[-1].strip() for li in agency_soup.find_all('li') if 'Agency:' in li.get_text()]
                agency = agency_li[0] if agency_li else "N/A"
                all_agencies.append(agency)
                
            # If the action date is before the specified start date, stop processing
            elif action_date < start_date:
                return pd.DataFrame({
                    "Title": all_titles,
                    "Date": all_dates,
                    "Category": all_categories,
                    "Link": all_links,
                    "Agency": all_agencies
                })
                
        # Go to the next page
        page_num += 1
        time.sleep(1)  # Pause to avoid server blocking

    # Save to DataFrame with Date as datetime format
    enforcement_action_df = pd.DataFrame({
        "Title": all_titles,
        "Date": all_dates,
        "Category": all_categories,
        "Link": all_links,
        "Agency": all_agencies
    })
    
    # Ensure Date is in datetime format
    enforcement_action_df['Date'] = pd.to_datetime(enforcement_action_df['Date'])
    
    # Save DataFrame to CSV
    file_name = f"enforcement_actions_{start_date.year}_{start_date.month}.csv"
    enforcement_action_df.to_csv(file_name, index=False)
    print(f"Data saved to {file_name}")
    
    return enforcement_action_df

# Define the start date
start_date = datetime(2023, 1, 1)

# Run the dynamic scraper function for all pages
df = scrape_enforcement_actions(start_date)

# Print the total number of enforcement actions
print(f"Total enforcement actions since January 2013: {len(df)}")

# Find the earliest enforcement action and print its details
earliest_action = df.sort_values(by="Date").iloc[0]
print(f"Earliest enforcement action:")
print(earliest_action)

```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```